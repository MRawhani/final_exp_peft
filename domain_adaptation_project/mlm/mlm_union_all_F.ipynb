{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/guest/Desktop/projects/third-experiments/domain_adaptation_project/mlm', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '', '/home/guest/.cache/pypoetry/virtualenvs/third-experments-xuKQSur9-py3.8/lib/python3.8/site-packages', '/home/guest/Desktop/projects/third-experiments/domain_adaptation_project/modules']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 03:07:35.155922: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-05 03:07:35.226546: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-05 03:07:36.060513: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from setup import setup_src_path\n",
    "print(setup_src_path())\n",
    "import data.processed as processed\n",
    "import config.config as config\n",
    "import utils.setup as setup\n",
    "import utils.functions as fn\n",
    "from importlib import reload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk,concatenate_datasets\n",
    "\n",
    "\n",
    "adapter_name=\"mlm_union_all_F\"\n",
    "dataset = load_from_disk(f\"{config.Config.DATASETS_SAVE_PATH}/datasets\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'promptID': 86113,\n",
       " 'pairID': '86113e',\n",
       " 'premise': \"'Wherever your leader is,' White said.\",\n",
       " 'premise_binary_parse': \"( ` ( ( ( Wherever ( your leader ) ) is ) ( , ( ' ( White ( said . ) ) ) ) ) )\",\n",
       " 'premise_parse': \"(ROOT (S (`` `) (S (NP (NP (NNP Wherever)) (NP (PRP$ your) (NN leader))) (VP (VBZ is))) (, ,) ('' ') (NP (NNP White)) (VP (VBD said)) (. .)))\",\n",
       " 'hypothesis': 'White said that was wherever your leader is.',\n",
       " 'hypothesis_binary_parse': '( White ( ( said ( that ( was ( wherever ( ( your leader ) is ) ) ) ) ) . ) )',\n",
       " 'hypothesis_parse': '(ROOT (S (NP (NNP White)) (VP (VBD said) (SBAR (IN that) (S (VP (VBD was) (SBAR (WHADVP (WRB wherever)) (S (NP (PRP$ your) (NN leader)) (VP (VBZ is)))))))) (. .)))',\n",
       " 'genre': 'fiction',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(config)\n",
    "\n",
    "    \n",
    "filtered_target = dataset['train'].filter(lambda example: example['genre'] == \"fiction\").shuffle(seed=42)\n",
    "#filtered_target = shuffled_filtered_target.train_test_split(test_size=0.1)\n",
    "\n",
    "\n",
    "filtered_test_target = dataset['validation_matched'].filter(lambda example: example['genre'] == \"fiction\")\n",
    "train_target = filtered_target\n",
    "test_target = filtered_test_target\n",
    "train_target[2]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/.cache/pypoetry/virtualenvs/third-experments-xuKQSur9-py3.8/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 109514298 || all params: 109514298 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "from adapters import AutoAdapterModel,init\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM\n",
    "\n",
    "mdlcfg = AutoConfig.from_pretrained(\n",
    "    config.Config.MODEL_NAME,\n",
    " \n",
    ")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    config.Config.MODEL_NAME,\n",
    ")\n",
    "init(model)\n",
    "reload(fn)\n",
    "fn.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Name                     Architecture         #Param      %Param  Active   Train\n",
      "--------------------------------------------------------------------------------\n",
      "mlm_union_all_F          union            17,539,712      16.107       1       1\n",
      "--------------------------------------------------------------------------------\n",
      "Full model                               108,891,648     100.000               0\n",
      "================================================================================\n",
      "trainable params: 18131840 || all params: 127054010 || trainable%: 14.2709702747674\n"
     ]
    }
   ],
   "source": [
    "from adapters import ConfigUnion, LoRAConfig, PrefixTuningConfig, SeqBnConfig,SeqBnInvConfig,AdapterConfig,LoRAConfig\n",
    "\n",
    "union_config = ConfigUnion(\n",
    "    PrefixTuningConfig(prefix_length=10),\n",
    "    LoRAConfig(r=8, alpha=16),\n",
    "\n",
    "    SeqBnInvConfig(reduction_factor=2),\n",
    ")\n",
    "model.add_adapter(adapter_name, config=union_config)\n",
    "\n",
    "model.train_adapter([adapter_name])\n",
    "model.active_adapters = adapter_name\n",
    "print(model.adapter_summary())\n",
    "fn.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "    num_rows: 77348\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "reload(processed)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.Config.TOKENIZER_NAME)\n",
    "\n",
    "tokenized_dataset= processed.tokenize_dataset(train_target,tokenizer)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 128\n",
    "# Slicing produces a list of lists for each feature\n",
    "tokenized_samples = tokenized_dataset[444:470]\n",
    "\n",
    "results = fn.group_texts(tokenized_samples, chunk_size)\n",
    "for chunk in results[\"labels\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "    num_rows: 17714\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we group texts and chunk them\n",
    "lm_datasets = tokenized_dataset.map(fn.group_texts, batched=True,fn_kwargs={'chunk_size': chunk_size})\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] [MASK] you any reason to believe [MASK] your mistress was [MASK] to take a sleeping powder [MASK] night? \" [SEP] your [MASK] was awake all night. [SEP] [CLS] do you want to see them? \" [SEP] you can't touch them. [SEP] [CLS]'wherever [MASK] leader is,'white said. [SEP] white said that was [MASK] your leader is. [SEP] [CLS] [MASK] the meantime [MASK] the young adventurers were [MASK] bolt upright [MASK] very stiff [MASK] ill at ease choral in a taxi which, [MASK] a [MASK]ym of originality [MASK] was [MASK] returning to the ritz via regent [MASK] s park. [SEP] the [MASK]s walked home instead of taking a [MASK]'\n",
      "\n",
      "'>>> . [SEP] [CLS] my brother lawrence is convinced that we are making a fuss over nothing. [SEP] my brother is wrong when [MASK] thinks we are making [MASK] fuss. [SEP] [CLS] at eleven o [MASK] clock. tuppence made up her mind [MASK] [SEP] tuppence made a decision at eleven o'clock [MASK] [SEP] [CLS] a [MASK] [MASK] he uttered a [MASK]. [SEP] ten years later, he [MASK]. [SEP] [CLS] they began again. [SEP] they started again. [SEP] [CLS] agatha [MASK] clarissa, [MASK] mallowan, dbe ( 15 september 1890 - 12 [MASK] 1976 ), commonly known as agatha christie [MASK] was an english crime [MASK] [MASK]. [SEP] the [MASK]'\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "samples = [lm_datasets[i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "dd=data_collator(samples)\n",
    "for chunk in dd[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 15942\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1772\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "downsampled_dataset = lm_datasets.train_test_split(\n",
    "    test_size=0.1, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/.cache/pypoetry/virtualenvs/third-experments-xuKQSur9-py3.8/lib/python3.8/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reload(fn)\n",
    "trainer = fn.train_mlm_model(model,adapter_name,data_collator,tokenizer, downsampled_dataset['train'],downsampled_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2191e68fb66e425ba7f2c039eef3ec45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 14.65\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada88cf520cb4fb39cb3c1f4de6cec44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1808, 'grad_norm': 1.8299555778503418, 'learning_rate': 9.960000000000001e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edff2126f515438f814e9489dd4ec9c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8095473051071167, 'eval_runtime': 9.6718, 'eval_samples_per_second': 183.212, 'eval_steps_per_second': 5.79, 'epoch': 1.0}\n",
      "{'loss': 1.8829, 'grad_norm': 1.5340802669525146, 'learning_rate': 9.476793248945148e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821362fd3ab247e2925984e3a25ea66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7311623096466064, 'eval_runtime': 9.8425, 'eval_samples_per_second': 180.036, 'eval_steps_per_second': 5.69, 'epoch': 2.0}\n",
      "{'loss': 1.7814, 'grad_norm': 1.6666128635406494, 'learning_rate': 8.951476793248945e-05, 'epoch': 2.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228598914dab4cd8ae33cad77b3c84bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.631156325340271, 'eval_runtime': 9.9466, 'eval_samples_per_second': 178.151, 'eval_steps_per_second': 5.63, 'epoch': 3.0}\n",
      "{'loss': 1.7153, 'grad_norm': 1.666502594947815, 'learning_rate': 8.426160337552744e-05, 'epoch': 3.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf6ef1b4f504e7cb4f634a681d7c571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.61770498752594, 'eval_runtime': 9.7296, 'eval_samples_per_second': 182.124, 'eval_steps_per_second': 5.756, 'epoch': 4.0}\n",
      "{'loss': 1.6909, 'grad_norm': 1.5208848714828491, 'learning_rate': 7.90084388185654e-05, 'epoch': 4.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389c1a20ba45486a94f8f246f9bbc682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.607940435409546, 'eval_runtime': 9.6362, 'eval_samples_per_second': 183.891, 'eval_steps_per_second': 5.811, 'epoch': 5.0}\n",
      "{'loss': 1.6575, 'grad_norm': 1.7393468618392944, 'learning_rate': 7.375527426160338e-05, 'epoch': 5.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6adf9c164047de8f0d35341aac1539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6065657138824463, 'eval_runtime': 9.986, 'eval_samples_per_second': 177.449, 'eval_steps_per_second': 5.608, 'epoch': 6.0}\n",
      "{'loss': 1.6277, 'grad_norm': 1.6114498376846313, 'learning_rate': 6.850210970464134e-05, 'epoch': 6.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebab621749f54a41bab58d4cbcdec143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5732533931732178, 'eval_runtime': 9.5762, 'eval_samples_per_second': 185.041, 'eval_steps_per_second': 5.848, 'epoch': 7.0}\n",
      "{'loss': 1.6252, 'grad_norm': 1.5389896631240845, 'learning_rate': 6.324894514767933e-05, 'epoch': 7.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272205865f2e441a8b4850dbc9a0dfa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5679792165756226, 'eval_runtime': 9.8355, 'eval_samples_per_second': 180.163, 'eval_steps_per_second': 5.694, 'epoch': 8.0}\n",
      "{'loss': 1.6038, 'grad_norm': 1.5224828720092773, 'learning_rate': 5.79957805907173e-05, 'epoch': 8.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c46db39a1e469ea0275e5918049090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.548417091369629, 'eval_runtime': 10.0016, 'eval_samples_per_second': 177.171, 'eval_steps_per_second': 5.599, 'epoch': 9.0}\n",
      "{'loss': 1.5911, 'grad_norm': 1.5900741815567017, 'learning_rate': 5.275316455696203e-05, 'epoch': 9.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f9e5535f774529af047d1fbf47d7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5474555492401123, 'eval_runtime': 9.7876, 'eval_samples_per_second': 181.046, 'eval_steps_per_second': 5.722, 'epoch': 10.0}\n",
      "{'loss': 1.5701, 'grad_norm': 1.6044648885726929, 'learning_rate': 4.75e-05, 'epoch': 10.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31751b98d6e646938ea987e028982252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5494242906570435, 'eval_runtime': 9.7264, 'eval_samples_per_second': 182.184, 'eval_steps_per_second': 5.758, 'epoch': 11.0}\n",
      "{'loss': 1.5642, 'grad_norm': 1.6784390211105347, 'learning_rate': 4.224683544303797e-05, 'epoch': 11.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5980f458654d94b60d64a51380a0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5089014768600464, 'eval_runtime': 9.9876, 'eval_samples_per_second': 177.42, 'eval_steps_per_second': 5.607, 'epoch': 12.0}\n",
      "{'loss': 1.555, 'grad_norm': 1.651281476020813, 'learning_rate': 3.699367088607595e-05, 'epoch': 12.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cfb9c204eb4bebab606206135eea07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.502734899520874, 'eval_runtime': 10.0207, 'eval_samples_per_second': 176.835, 'eval_steps_per_second': 5.588, 'epoch': 13.0}\n",
      "{'loss': 1.5453, 'grad_norm': 1.697893500328064, 'learning_rate': 3.174050632911393e-05, 'epoch': 13.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9069dd3d7546de8cbabd2029c12a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5193445682525635, 'eval_runtime': 9.8883, 'eval_samples_per_second': 179.202, 'eval_steps_per_second': 5.663, 'epoch': 14.0}\n",
      "{'loss': 1.5378, 'grad_norm': 1.7475504875183105, 'learning_rate': 2.6487341772151903e-05, 'epoch': 14.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73accad5e9cf45d098f8f628359983f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4670168161392212, 'eval_runtime': 9.9695, 'eval_samples_per_second': 177.742, 'eval_steps_per_second': 5.617, 'epoch': 15.0}\n",
      "{'loss': 1.5273, 'grad_norm': 1.8724439144134521, 'learning_rate': 2.1234177215189873e-05, 'epoch': 15.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f4477a935248eb8cc04fd85d5295d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4967280626296997, 'eval_runtime': 9.7068, 'eval_samples_per_second': 182.552, 'eval_steps_per_second': 5.769, 'epoch': 16.0}\n",
      "{'loss': 1.5271, 'grad_norm': 1.681781530380249, 'learning_rate': 1.5981012658227848e-05, 'epoch': 16.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b74d64924d4185a16594eb60f7d7f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5001611709594727, 'eval_runtime': 6.3955, 'eval_samples_per_second': 277.07, 'eval_steps_per_second': 8.756, 'epoch': 17.0}\n",
      "{'loss': 1.5202, 'grad_norm': 1.7054705619812012, 'learning_rate': 1.0738396624472575e-05, 'epoch': 17.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57cafc9d51634d2abcc504f2b4d5bed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.48793625831604, 'eval_runtime': 6.4175, 'eval_samples_per_second': 276.119, 'eval_steps_per_second': 8.726, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwriting existing adapter 'mlm_union_all_F'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3305.9214, 'train_samples_per_second': 96.445, 'train_steps_per_second': 3.019, 'train_loss': 1.6499648999859935, 'epoch': 18.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8982, training_loss=1.6499648999859935, metrics={'train_runtime': 3305.9214, 'train_samples_per_second': 96.445, 'train_steps_per_second': 3.019, 'train_loss': 1.6499648999859935, 'epoch': 18.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738e3738d31744cf8688d6d0a97fe507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 4.51\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import config.config as config\n",
    "\n",
    "trainer.model.save_adapter(f\"{config.Config.ADAPTER_SAVE_PATH}/{adapter_name}\", adapter_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a report of mlm for Slate domain using whole genre as target and then splitt after tokenization with seed 42. I used computer metrix for preplexity and early stopping \n",
    "The results before:\n",
    ">>> Perplexity: 14.65\n",
    "\n",
    "The results after:\n",
    ">>> Perplexity: 5.68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intial-experments-_CPDD38x-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
